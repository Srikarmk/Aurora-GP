{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWDpTD6bvSQpGPMR6VxH2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srikarmk/Aurora-GP/blob/main/AURORA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGTY4Hn_4KA5",
        "outputId": "5ecf16e0-82ca-40a3-d583-5a1057321602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.14.2-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting jaxtyping (from gpytorch)\n",
            "  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from gpytorch) (1.16.3)\n",
            "Collecting linear-operator>=0.6 (from gpytorch)\n",
            "  Downloading linear_operator-0.6-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from linear-operator>=0.6->gpytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy>=1.6.0->gpytorch) (2.0.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping->gpytorch)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gpytorch) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->linear-operator>=0.6->gpytorch) (3.0.3)\n",
            "Downloading gpytorch-1.14.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.9/279.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.6-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, jaxtyping, linear-operator, gpytorch\n",
            "Successfully installed gpytorch-1.14.2 jaxtyping-0.3.3 linear-operator-0.6 wadler-lindig-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install gpytorch\n",
        "import torch\n",
        "import numpy as np\n",
        "import gpytorch\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def arr_to_tensor(arr):\n",
        "  return torch.tensor(arr, dtype = torch.float32).to(device)"
      ],
      "metadata": {
        "id": "K03OsY625a8M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class exactGP(gpytorch.models.ExactGP):\n",
        "  def __init__(self, train_x, train_y, likelihood):\n",
        "    super(exactGP, self).__init__(train_x, train_y, likelihood)\n",
        "    self.mean_module = gpytorch.means.ConstantMean()\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    cov_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, cov_x)"
      ],
      "metadata": {
        "id": "tH2oqSRW67PS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpytorch.likelihoods import likelihood\n",
        "def train_exactGP(train_x, train_y, epochs=10):\n",
        "  likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
        "  model = exactGP(train_x, train_y, likelihood).to(device)\n",
        "\n",
        "  model.train()\n",
        "  likelihood.train()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
        "  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "  start_time = time.time()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 10 == 0:\n",
        "      print(f\"Epoch: {i+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "  training_time = time.time() - start_time\n",
        "  print(f\"Training time - {training_time:.2f}s\")\n",
        "\n",
        "  return model, likelihood, training_time"
      ],
      "metadata": {
        "id": "PZGUG3NQCWJa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gp_eval(model, likelihood, test_x, test_y):\n",
        "  model.eval()\n",
        "  likelihood.eval()\n",
        "\n",
        "  with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
        "    observed_pred = likelihood(model(test_x))\n",
        "    mean_pred = observed_pred.mean\n",
        "    var_pred = observed_pred.variance\n",
        "    std_pred = var_pred.sqrt()\n",
        "    lower = mean_pred - 2*std_pred\n",
        "    upper = mean_pred + 2*std_pred\n",
        "\n",
        "    rmse = torch.sqrt(torch.mean((mean_pred - test_y)**2)).item()\n",
        "    nll = -observed_pred.log_prob(test_y).mean().item()\n",
        "    calibration = ((test_y >= lower) & (test_y <= upper)).float().mean().item()\n",
        "\n",
        "\n",
        "    return rmse, nll, calibration"
      ],
      "metadata": {
        "id": "DlHjIJ7AE_Dc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#def load_data(filepath, subset_size=None, test_size=0.2):\n",
        " # data = np.load(filepath)\n",
        "  #X = data['X']\n",
        "  #y = data['y']\n",
        "  #ft_names = data['feature_names']\n",
        "\n",
        "  #print(f\"Features: {ft_names}\")\n",
        "  #print(f\"Shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "  #if subset_size is not None and subset_size < len(X):\n",
        "   # indices = np.random.RandomState(42).choice(len(X), size=subset_size, replace=False)\n",
        "    #X = X[indices]\n",
        "    #y = y[indices]\n",
        "    #print(f\"Subset: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "  #x_scale = StandardScaler()\n",
        "  #y_scale = StandardScaler()\n",
        "  #X_scaled = x_scale.fit_transform(X)\n",
        "  #y_scaled = y_scale.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "  #X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_scaled, y_scaled, test_size=test_size, random_state=42)\n",
        "\n",
        "  #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  #X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
        "  #X_test = torch.tensor(X_test_np, dtype=torch.float32).to(device)\n",
        "  #y_train = torch.tensor(y_train_np, dtype=torch.float32).to(device)\n",
        "  #y_test = torch.tensor(y_test_np, dtype=torch.float32).to(device)\n",
        "\n",
        "  #return X_train, X_test, y_train, y_test, x_scale, y_scale"
      ],
      "metadata": {
        "id": "yjyEqSXUo2Un"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "def load_data(filepath, subset_size=None, test_size=0.2, random_state=42):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  data = np.load(filepath)\n",
        "  X = data['X']\n",
        "  y = data['y']\n",
        "  ft_names = data['feature_names']\n",
        "  print(f\"Shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "  if subset_size is not None and subset_size < len(X):\n",
        "    indices = np.random.RandomState(random_state).choice(len(X), size=subset_size, replace=False)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "    print(f\"Subset: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "  x_scale = StandardScaler()\n",
        "  y_scale = StandardScaler()\n",
        "  X_train = x_scale.fit_transform(X_train)\n",
        "  X_test = x_scale.transform(X_test)\n",
        "  y_train = y_scale.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "  y_test = y_scale.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "  train_x = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "  test_x = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "  train_y = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "  test_y = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "  return train_x, test_x, train_y, test_y, x_scale, y_scale"
      ],
      "metadata": {
        "id": "EVZBm-_WzDj7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daoL_Abyr6fI",
        "outputId": "e087e4fa-6f7b-4e7e-bc42-0e975d2ced67"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ROBOT ARM\")\n",
        "load_data(filepath=\"/content/drive/MyDrive/datasets/robot_arm.npz\", subset_size=None, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2teOYg6GsCOa",
        "outputId": "49cfe138-51be-4179-e056-6e291795f269"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROBOT ARM\n",
            "Shape: X=(8192, 8), y=(8192,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.8444, -0.5209,  0.8397,  ..., -1.5918,  0.2164, -1.5939],\n",
              "         [ 1.3718, -1.5744,  0.8551,  ...,  0.8812, -0.9470, -0.8579],\n",
              "         [ 1.6893,  0.0151, -1.6489,  ..., -1.0079,  0.7043, -1.0411],\n",
              "         ...,\n",
              "         [-0.9737, -0.1102,  0.0978,  ...,  0.1581, -0.2764, -0.5299],\n",
              "         [ 1.0023, -0.0682,  1.3690,  ..., -1.3995, -1.0722, -0.9902],\n",
              "         [-0.0949,  1.0117, -0.8734,  ..., -1.1463,  0.8837,  0.3538]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 0.2230, -0.4538, -1.4178,  ...,  0.1198, -0.5327, -0.5034],\n",
              "         [-1.5587,  1.0166, -0.8154,  ...,  0.4192, -0.8474,  1.0298],\n",
              "         [ 0.1362,  0.5293,  0.2273,  ...,  0.0099,  0.1104,  1.6830],\n",
              "         ...,\n",
              "         [-0.9282,  1.5452,  0.3153,  ...,  0.0812,  0.0172,  0.0926],\n",
              "         [ 0.7231,  1.6143,  0.4003,  ..., -0.8685, -0.0963, -1.7352],\n",
              "         [-1.0041, -0.2028,  1.2649,  ..., -0.0769,  1.6064, -0.4365]],\n",
              "        device='cuda:0'),\n",
              " tensor([-0.4431,  0.0979,  0.8685,  ..., -0.3213, -0.6257,  1.8739],\n",
              "        device='cuda:0'),\n",
              " tensor([ 1.0299,  1.5667, -0.0328,  ...,  0.4973, -1.2393, -1.2242],\n",
              "        device='cuda:0'),\n",
              " StandardScaler(),\n",
              " StandardScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_sizes = [1000, 2000, 5000]\n",
        "\n",
        "for subset_size in subset_sizes:\n",
        "  train_x, test_x, train_y, test_y, _, _ = load_data(filepath=\"/content/drive/MyDrive/datasets/robot_arm.npz\", subset_size=subset_size)\n",
        "  model, likelihood, training_time = train_exactGP(train_x, train_y, epochs=10)\n",
        "  metrics = gp_eval(model, likelihood, test_x, test_y)\n",
        "  print(f\"Subset Size: {subset_size}\")\n",
        "  print(f\"Training Time: {training_time:.2f}s\")\n",
        "  print(f\"RMSE: {metrics[0]:.4f}\")\n",
        "  print(f\"NLL: {metrics[1]:.4f}\")\n",
        "  print(f\"Calibration: {metrics[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGfYqeLeuQZx",
        "outputId": "731defa2-71b4-441b-a3ef-d706fb601958"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: X=(8192, 8), y=(8192,)\n",
            "Subset: X=(1000, 8), y=(1000,)\n",
            "Epoch: 10/10, Loss: 1.0269055366516113\n",
            "Training time - 0.09s\n",
            "Subset Size: 1000\n",
            "Training Time: 0.09s\n",
            "RMSE: 0.5091\n",
            "NLL: 157.3750\n",
            "Calibration: 0.9900\n",
            "Shape: X=(8192, 8), y=(8192,)\n",
            "Subset: X=(2000, 8), y=(2000,)\n",
            "Epoch: 10/10, Loss: 0.9014069437980652\n",
            "Training time - 0.23s\n",
            "Subset Size: 2000\n",
            "Training Time: 0.23s\n",
            "RMSE: 0.4153\n",
            "NLL: 286.7465\n",
            "Calibration: 0.9975\n",
            "Shape: X=(8192, 8), y=(8192,)\n",
            "Subset: X=(5000, 8), y=(5000,)\n",
            "Epoch: 10/10, Loss: 0.7753798961639404\n",
            "Training time - 0.48s\n",
            "Subset Size: 5000\n",
            "Training Time: 0.48s\n",
            "RMSE: 0.3314\n",
            "NLL: 646.2325\n",
            "Calibration: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CONCRETE\")\n",
        "load_data(filepath=\"/content/drive/MyDrive/datasets/concrete.npz\", subset_size=None, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fu1EIFA69Yg6",
        "outputId": "71b3583f-8c22-46a9-f392-8c5f89c6b055"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONCRETE\n",
            "Shape: X=(1030, 8), y=(1030,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.1608,  0.8574,  0.9825,  ..., -0.2575, -0.6475, -0.2757],\n",
              "         [ 1.3086, -0.6025,  1.2326,  ..., -1.9269, -0.2731, -0.2757],\n",
              "         [-0.0771, -0.8556,  1.0661,  ...,  1.0179,  0.0666, -0.6893],\n",
              "         ...,\n",
              "         [-0.8655, -0.8556,  1.1260,  ...,  1.3408,  0.3311,  0.9156],\n",
              "         [ 1.7832,  0.5111, -0.8311,  ..., -1.5442,  0.1161, -0.2757],\n",
              "         [ 0.2851, -0.8556,  0.9356,  ..., -0.6173,  0.1346, -0.2757]],\n",
              "        device='cuda:0'),\n",
              " tensor([[-0.1615,  0.4559, -0.8311,  ..., -0.5283, -1.2616,  5.3002],\n",
              "         [ 0.7373,  1.3187, -0.8311,  ..., -0.3668, -0.2015, -0.6231],\n",
              "         [ 0.9913,  1.3187, -0.8311,  ..., -0.3668, -0.2015, -0.2757],\n",
              "         ...,\n",
              "         [ 0.0274,  1.4556, -0.8311,  ...,  0.3948,  0.3904, -0.6231],\n",
              "         [ 1.3086, -0.6025,  1.2326,  ..., -1.9269, -0.2731,  0.1876],\n",
              "         [ 0.5177, -0.8556, -0.8311,  ..., -0.0705,  0.1099,  0.7502]],\n",
              "        device='cuda:0'),\n",
              " tensor([-4.8512e-01,  1.5542e+00, -7.1535e-01, -1.4592e-01, -1.6885e+00,\n",
              "         -4.7972e-01, -1.0424e+00,  7.5526e-01,  2.2797e-01, -1.8566e+00,\n",
              "         -1.3387e+00,  6.5975e-02, -7.9062e-01,  2.1027e+00, -6.0805e-01,\n",
              "          2.3956e+00,  1.6065e+00, -1.0868e+00,  3.0827e-01, -8.3054e-01,\n",
              "          6.6894e-01, -1.1483e+00, -7.7957e-01, -3.6396e-01, -9.4484e-01,\n",
              "         -1.5568e+00,  1.8951e-01,  3.7237e-01,  2.0551e-01, -8.1290e-02,\n",
              "         -4.6050e-01,  1.6292e-01,  1.0629e+00, -1.6934e+00, -1.8004e-01,\n",
              "          4.4612e-01,  1.1451e+00, -3.7520e-02,  1.8299e+00, -2.9523e-01,\n",
              "          2.4292e+00, -1.0616e+00,  1.1395e+00,  1.6875e+00, -1.2216e-01,\n",
              "          5.9817e-01,  8.3155e-02, -6.5317e-01,  2.5456e-01, -1.3240e+00,\n",
              "         -1.7640e-01, -1.0847e+00, -8.4952e-01, -1.0692e+00, -7.3883e-01,\n",
              "          1.1684e+00, -1.0834e+00, -2.6329e-01, -1.3077e+00,  1.7830e+00,\n",
              "          2.7229e+00, -4.1877e-01,  4.1164e-01,  3.7220e-01, -1.3363e+00,\n",
              "         -1.3874e+00, -5.0059e-01, -2.2651e-01,  2.0997e-01, -1.4592e-01,\n",
              "         -4.4863e-01, -1.2160e+00,  3.1101e-01, -7.1326e-01,  4.3864e-01,\n",
              "         -7.6422e-02, -1.5210e+00,  1.9197e-01, -6.6462e-01, -7.6607e-01,\n",
              "          6.6285e-01, -1.0520e+00, -1.4592e-01,  5.8386e-01, -2.8133e-01,\n",
              "         -8.9779e-01,  4.1839e-02, -1.3883e+00, -1.8969e-01,  1.5679e-01,\n",
              "         -6.3353e-01, -1.0622e+00, -9.8836e-01, -5.7299e-01, -1.2066e+00,\n",
              "         -1.4502e+00,  1.4499e+00,  2.0160e+00, -6.4908e-01,  2.6295e-02,\n",
              "         -2.0933e-01,  2.0363e-01,  1.8372e+00, -6.0122e-01, -1.0868e+00,\n",
              "         -6.3231e-01, -7.2493e-01, -1.2465e-01, -1.3388e+00, -1.3351e+00,\n",
              "          9.0170e-01, -6.7485e-01,  1.7828e+00, -6.1018e-01,  4.8486e-01,\n",
              "         -1.7824e-01, -1.3290e+00,  1.3388e-01,  2.3216e+00,  2.5773e+00,\n",
              "         -1.2063e+00,  6.2476e-01, -1.4048e+00, -1.5431e+00, -1.2792e-01,\n",
              "         -7.4831e-01,  3.5008e-02,  2.4586e+00, -6.1099e-01,  1.5446e-01,\n",
              "         -7.3171e-01, -8.3054e-01,  1.1588e-01, -9.5915e-01,  9.3382e-02,\n",
              "         -5.8199e-01,  1.8010e-01, -5.7610e-01, -1.4461e+00, -1.6597e-01,\n",
              "         -1.1965e+00,  1.8659e+00, -4.2900e-01, -2.2483e-01,  2.1324e-01,\n",
              "          2.0015e-01,  2.0874e-01, -5.0550e-01, -2.5637e-01,  3.2287e-01,\n",
              "          1.3611e+00,  5.4745e-01,  9.4138e-01,  1.1660e+00, -7.1248e-01,\n",
              "          1.9515e+00,  1.0232e+00, -1.2357e+00,  1.8892e+00, -1.0847e+00,\n",
              "          2.4228e-01, -6.7403e-01, -1.4180e+00,  1.8544e+00,  2.6182e+00,\n",
              "         -7.4235e-01, -1.1361e-01, -1.0868e+00, -1.3509e+00,  1.2678e+00,\n",
              "         -9.4156e-01, -7.1870e-01,  5.3460e-01,  1.7407e+00,  1.2187e+00,\n",
              "         -1.3687e+00, -7.2152e-01, -1.2874e-01, -1.8613e-01,  2.8114e-01,\n",
              "         -4.0740e-01,  2.7732e+00,  1.6417e+00, -1.1056e+00, -3.5520e-01,\n",
              "          2.1436e+00, -2.7233e-01,  9.6961e-01, -7.0245e-02, -1.7437e+00,\n",
              "          8.7470e-01,  2.1620e+00,  5.3436e-01, -2.3629e-01, -1.8437e+00,\n",
              "          1.7882e+00,  1.0445e+00,  1.7407e+00,  9.3137e-02, -5.3065e-01,\n",
              "         -7.7200e-02, -3.7173e-01, -1.4515e+00,  7.7919e-02,  1.7407e+00,\n",
              "         -6.3727e-01,  1.7231e+00,  1.9129e+00,  4.6519e-01, -1.2553e+00,\n",
              "          4.6727e-01,  1.7483e-01,  2.6295e-02, -1.5523e+00, -1.9726e-01,\n",
              "          8.3748e-01,  1.9841e+00, -3.7214e-01, -1.3453e+00, -6.1553e-01,\n",
              "         -1.5670e+00, -1.9891e+00,  2.1027e+00, -1.8331e-01,  2.0219e-01,\n",
              "          8.8252e-01, -2.4001e-01, -6.7690e-01,  4.6519e-01,  1.3961e-01,\n",
              "          6.6690e-01, -1.2792e-01,  3.6055e-01, -2.9464e-04,  9.2973e-02,\n",
              "         -2.3481e-01,  8.2562e-01,  3.1714e-01,  9.8838e-01, -1.2381e+00,\n",
              "          1.2150e+00, -6.8017e-01, -1.5024e+00, -6.6912e-01,  1.2318e+00,\n",
              "          1.3496e+00,  6.5704e-01,  2.5478e+00,  1.0951e+00,  1.3989e-01,\n",
              "         -2.8681e-01,  9.3382e-02, -9.6938e-01, -1.3288e+00,  4.7668e-01,\n",
              "         -8.5300e-01, -1.5329e-01,  7.9453e-01, -1.2010e+00,  7.1844e-01,\n",
              "          2.7918e-01, -1.3776e+00,  5.1396e-01, -6.7755e-01,  1.2815e-01,\n",
              "          1.0739e+00, -3.3020e-02,  9.4187e-01,  1.1508e+00,  4.0591e-01,\n",
              "         -8.4584e-01,  5.8414e-03, -3.2100e-01, -8.6588e-01,  4.5827e-01,\n",
              "          4.2452e-01,  2.5773e+00,  1.2326e+00, -6.1385e-03, -9.2438e-01,\n",
              "         -1.3733e-01, -8.8347e-01, -7.4971e-01, -5.6571e-01, -6.3132e-01,\n",
              "          6.4235e-01,  8.9475e-01, -6.8344e-01, -5.3454e-01,  1.2245e+00,\n",
              "         -8.0154e-01, -1.1332e-01, -1.4533e+00,  4.4682e-01, -1.2614e+00,\n",
              "          5.0245e-01,  1.8237e+00,  5.0082e-01, -1.3013e-01, -4.0282e-01,\n",
              "          5.6050e-01, -6.0735e-01, -1.2066e+00, -4.0691e-01,  6.1617e-01,\n",
              "          1.3787e+00,  1.3087e+00, -1.6699e-01, -7.6894e-01, -1.4050e+00,\n",
              "          2.0424e-01,  8.2562e-01, -3.5537e-01, -1.2848e+00, -1.6720e+00,\n",
              "         -4.6492e-01,  3.2696e-01, -1.1048e+00,  1.8616e-01,  2.2745e+00,\n",
              "         -1.2013e+00,  8.1519e-02, -3.7418e-01, -5.3045e-01,  1.2673e+00,\n",
              "         -1.6306e+00, -1.1858e+00,  3.9691e-01,  1.7274e-01,  1.0116e+00,\n",
              "         -9.0561e-01, -8.3520e-01, -1.8601e-01, -4.2900e-01,  2.5773e+00,\n",
              "          1.1296e+00, -6.9571e-01, -1.6490e+00,  1.6188e+00, -1.4874e+00,\n",
              "          9.8393e-01,  6.7202e-02, -8.9779e-01, -8.7088e-01,  1.2202e-01,\n",
              "          2.7951e-01, -1.4124e+00,  6.1372e-01,  1.1604e-01,  2.9661e-01,\n",
              "         -1.5815e-01, -1.3191e+00, -1.4470e-01,  1.9606e-01,  2.6308e+00,\n",
              "          1.1506e-01, -2.6742e-01,  1.0600e+00,  2.8810e-01, -1.2629e-01,\n",
              "          2.5946e-01,  6.4235e-01,  2.1226e-01,  1.4204e+00, -1.7906e-01,\n",
              "         -2.2236e-01, -1.1069e+00,  8.6939e-01,  1.4499e+00,  1.4973e+00,\n",
              "          8.9966e-01, -4.7645e-01, -1.5857e+00, -1.5501e+00, -5.7103e-01,\n",
              "         -1.2173e+00,  7.0944e-01, -1.2346e+00, -1.0970e+00, -6.4499e-01,\n",
              "          6.5503e-01,  4.9959e-01, -1.4243e+00, -2.6742e-01,  7.0494e-01,\n",
              "          8.3565e-02, -1.9305e+00, -7.5462e-01,  6.8490e-01, -1.5347e+00,\n",
              "         -3.2243e-02,  1.6875e+00,  8.8043e-01,  7.4421e-01,  1.5055e+00,\n",
              "          2.3267e-01,  1.3909e+00, -1.3882e+00,  1.7231e+00,  2.5456e-01,\n",
              "          2.0424e-01, -6.1684e-01, -4.2573e-01,  4.0125e-01, -1.2038e+00,\n",
              "         -1.7169e-01, -3.4023e-01, -9.7552e-01,  4.7967e-01,  5.0613e-01,\n",
              "          3.7801e-01,  2.5773e+00, -6.0326e-01,  5.6198e-02, -1.6692e+00,\n",
              "         -1.1625e+00, -7.3212e-01,  1.1296e+00, -1.7066e-02, -1.0986e+00,\n",
              "         -7.9315e-01, -1.4472e+00,  1.2035e+00,  1.1651e+00,  3.6623e-01,\n",
              "         -5.6645e-01, -1.2124e+00, -1.0575e+00, -8.1393e-01, -4.4883e-02,\n",
              "         -9.2744e-02, -1.3658e+00,  2.2214e+00, -6.6564e-02,  2.1528e-01,\n",
              "          6.3929e-02,  2.0054e+00,  2.2572e-01,  3.7196e-01, -1.2333e+00,\n",
              "          8.3565e-02, -4.6356e-02,  3.4763e-02,  2.4586e+00,  3.0123e-01,\n",
              "          1.3848e+00,  2.0768e-01,  1.7704e+00, -5.6956e-01,  2.9730e-01,\n",
              "          5.6157e-02, -8.2416e-01, -1.5288e+00,  2.0424e-01,  4.9550e-01,\n",
              "          6.3929e-02,  3.3719e-01, -1.0700e+00,  3.1407e-01,  8.7246e-02,\n",
              "          1.3920e-01,  2.9199e-02, -1.0586e+00, -1.0598e+00,  1.1060e-01,\n",
              "         -1.1717e+00,  8.9332e-02, -1.1616e+00,  4.9959e-01,  4.0264e-01,\n",
              "         -5.7136e-01,  2.8483e-01,  2.2729e+00, -1.3887e+00, -1.2711e-01,\n",
              "         -1.3445e+00,  1.4081e+00,  4.7112e-01, -2.3633e-01, -7.8481e-01,\n",
              "          2.4944e-01, -1.1025e+00,  1.7231e+00, -1.5928e+00,  2.4586e+00,\n",
              "          1.1119e+00,  2.0915e-01, -2.6169e-01,  1.1889e+00, -6.8917e-01,\n",
              "         -1.5131e+00,  7.9166e-01, -9.6017e-02, -1.1261e+00, -1.2848e+00,\n",
              "         -1.2647e+00, -4.7072e-01, -1.7156e+00,  1.5390e+00,  7.9072e-01,\n",
              "          2.2451e+00,  1.3730e+00, -1.5497e+00, -9.8062e-02, -5.9115e-01,\n",
              "         -5.2758e-01, -1.0520e+00,  5.9310e-01,  6.1642e-01, -3.4281e-01,\n",
              "          7.7898e-01, -1.8365e+00,  3.8505e-01,  7.7816e-01, -2.3837e-01,\n",
              "          2.2793e-01, -1.3834e+00, -2.9073e-01,  4.8854e-01,  5.0654e-01,\n",
              "         -6.3926e-01,  1.9524e-01, -1.3829e+00,  1.7943e+00,  1.3603e+00,\n",
              "         -3.3900e-01, -5.1163e-01, -4.8148e-01, -6.3353e-01, -7.7753e-01,\n",
              "          6.5647e-02,  1.1476e+00,  1.6875e+00,  9.9415e-01,  6.1564e-01,\n",
              "         -1.2381e+00, -3.9312e-01,  8.4362e-01,  1.1987e+00,  3.8178e-01,\n",
              "         -6.5071e-01,  1.4499e+00, -9.9965e-01, -8.2539e-01,  1.4441e+00,\n",
              "          5.1939e-01, -1.1238e-01, -6.9735e-01, -1.3490e+00,  7.6016e-01,\n",
              "          1.8990e+00,  1.8417e+00,  1.8949e+00, -3.0566e-02,  3.2942e-01,\n",
              "         -3.3369e-01, -1.6239e+00,  9.2973e-02,  6.7426e-01,  9.8421e-01,\n",
              "          3.4291e-01,  2.3656e-01,  2.1027e+00, -5.8117e-01,  1.0514e+00,\n",
              "         -4.2884e-01, -9.5915e-01,  2.9669e-01, -1.0619e+00, -1.4889e+00,\n",
              "         -1.3719e+00, -9.9961e-01, -3.8359e-01,  2.8605e-01, -7.3175e-01,\n",
              "         -4.9404e-01, -2.0524e-01, -1.2220e-01,  1.2445e+00, -1.5136e-01,\n",
              "          2.1502e+00,  5.4745e-01,  4.0223e-01, -5.1245e-01,  1.5190e+00,\n",
              "          1.2007e+00, -1.7235e+00,  8.8493e-01, -7.1617e-01, -1.0348e+00,\n",
              "         -9.1616e-01,  6.1535e-01,  2.1348e-01, -2.6333e-01,  1.0706e+00,\n",
              "          3.8955e-01, -1.1931e+00,  4.9141e-01, -3.5393e-01, -8.6425e-01,\n",
              "         -7.6689e-01, -2.2279e-01,  1.8446e+00,  4.1900e-01, -1.2672e+00,\n",
              "         -1.6623e+00, -3.1364e-01,  4.6339e-02, -2.3060e-01,  3.2696e-01,\n",
              "         -1.6677e+00,  1.8976e-01,  1.0064e+00,  5.0818e-01,  6.3990e-01,\n",
              "          1.2243e-01,  4.6523e-01, -2.7760e-01,  9.0088e-01, -7.1617e-01,\n",
              "         -4.1055e-01, -4.8831e-01, -1.6210e+00, -1.2128e+00, -9.6819e-01,\n",
              "         -6.1103e-01, -1.9170e-01, -1.2037e+00, -1.0477e+00,  1.5464e+00,\n",
              "         -4.3064e-01, -2.6742e-01,  3.4570e-01, -7.1903e-01,  2.5834e+00,\n",
              "          1.4024e+00,  1.5131e-01, -6.7951e-01,  1.2899e+00, -6.8626e-01,\n",
              "         -3.0914e-01, -2.1465e-01,  3.4476e-02, -4.0253e-01, -1.3365e-01,\n",
              "          7.9166e-01, -8.3287e-01, -8.0330e-01,  1.0927e+00, -1.6821e+00,\n",
              "         -7.3179e-01,  8.3871e-01, -6.1913e-01, -1.5165e-01,  1.1443e-04,\n",
              "         -6.4990e-01,  6.7753e-01, -5.4067e-01, -3.9750e-01, -4.7072e-01,\n",
              "         -1.7210e-01,  1.6047e-01,  5.0065e-01,  1.8624e-01, -8.8920e-01,\n",
              "         -7.1003e-01, -5.8690e-01, -1.0054e+00, -2.3085e-01,  1.6865e-01,\n",
              "         -1.6210e+00,  4.3353e-02,  1.1713e+00,  2.2101e-01, -1.0929e+00,\n",
              "          1.6865e-01, -1.2733e+00, -1.4055e-01,  2.9080e-01,  3.4778e-01,\n",
              "         -6.3353e-01, -5.3413e-01, -8.7775e-01,  2.1027e+00,  9.6511e-01,\n",
              "         -4.0773e-01, -3.2469e-01, -6.1390e-01, -7.0348e-01,  4.2718e-01,\n",
              "          5.6422e-01, -1.4165e+00, -1.3226e+00,  1.1741e+00,  9.3750e-02,\n",
              "          1.1627e+00,  8.3298e-01, -1.5421e+00,  1.2560e+00,  4.4612e-01,\n",
              "          1.9366e+00,  8.3524e-02, -1.3327e+00, -4.8545e-01,  1.0068e+00,\n",
              "          7.6168e-01, -6.9836e-02,  7.0936e-01,  2.2034e+00,  8.6632e-01,\n",
              "          5.9572e-01,  1.8814e+00, -3.8032e-01, -1.7556e+00, -4.3841e-01,\n",
              "         -1.5498e+00,  3.0814e-01, -1.6261e-01, -2.8664e-01,  1.2453e+00,\n",
              "         -1.5697e-01,  4.5729e-01, -5.6125e-03, -7.3253e-01,  8.7033e-01,\n",
              "         -3.9382e-01, -1.2786e+00,  1.0559e+00,  9.1357e-01, -5.3904e-01,\n",
              "         -1.2220e-01, -3.7520e-02, -1.2115e+00, -5.6113e-01,  1.4261e+00,\n",
              "         -4.7003e-01,  1.2202e-01,  1.3447e+00, -1.1090e+00, -6.4294e-01,\n",
              "         -8.9493e-01, -1.3253e+00,  1.0821e+00, -1.2468e+00,  4.9100e-01,\n",
              "          1.5444e+00,  4.8259e-04,  1.2218e-01, -1.6466e-01, -1.2173e+00,\n",
              "         -1.4592e-01, -8.8634e-01,  1.0581e+00,  3.0066e-01, -7.9634e-01,\n",
              "         -2.2820e-01, -9.7552e-01,  2.2469e-01,  1.5057e+00,  9.6961e-01,\n",
              "         -3.3020e-02, -8.2743e-01,  3.0119e-01, -8.7758e-01, -1.2629e-01,\n",
              "         -8.7202e-01, -6.7158e-01,  3.8382e-01, -1.4838e-01, -5.2308e-01,\n",
              "          8.9966e-01,  5.9204e-01,  1.5808e+00,  5.6013e-01,  3.5474e-01,\n",
              "          7.0347e-01,  1.1917e+00,  1.1218e+00,  3.3727e-01, -3.7430e-01,\n",
              "         -1.1486e+00,  2.9669e-01, -2.2581e-01, -1.0426e+00, -6.9964e-01,\n",
              "         -1.4466e-01, -1.2883e+00, -3.6379e-01, -1.5114e+00, -8.4298e-01,\n",
              "          4.5807e-01, -1.5602e+00, -2.9339e-02, -6.8679e-01,  1.8671e-01,\n",
              "          8.9536e-01,  1.0400e+00, -5.9999e-01,  2.9710e-01, -5.8117e-01,\n",
              "          1.4656e-01, -4.5211e-01,  9.6020e-01,  1.1369e+00,  2.4586e+00,\n",
              "         -3.6805e-01, -2.1432e-01, -7.7262e-01,  1.4490e+00,  7.6753e-01,\n",
              "         -1.1890e+00,  5.8202e-02,  1.2392e+00,  3.1960e-01,  1.1737e+00,\n",
              "         -1.5108e+00,  2.3044e+00, -4.7363e-01, -1.8356e-01, -6.7976e-01,\n",
              "          3.7728e-01, -1.3611e-01,  1.9248e+00,  1.5433e-01], device='cuda:0'),\n",
              " tensor([ 1.0116,  1.1889,  2.2925, -0.0330, -1.5024,  0.5000, -0.7219,  0.5642,\n",
              "          0.0917,  0.7706, -0.6384, -1.4908,  0.2574,  0.8273, -0.1692, -0.8271,\n",
              "          0.2492, -0.9794,  0.0778, -0.1643, -0.2656,  0.1245,  0.7630, -1.7179,\n",
              "          0.1629, -0.2973, -1.4648,  0.3081,  1.0146, -1.3429,  0.9757,  0.2223,\n",
              "          0.9496,  1.8135, -1.2410, -0.1684, -0.2502,  0.2114, -1.3168,  0.9553,\n",
              "         -1.2034, -1.7548,  0.1315,  0.8530, -1.5323,  1.6339,  1.1684,  0.0173,\n",
              "         -0.8368, -1.8410,  1.2670,  0.7075, -0.3824, -1.3934,  1.6708, -0.0866,\n",
              "         -0.6221, -1.3138, -0.2457, -0.9548,  0.2112, -1.3253, -0.0306,  0.6493,\n",
              "         -0.1693, -1.0577, -0.1458, -1.4050, -0.4863, -0.6781, -1.4360, -0.7008,\n",
              "         -1.5752, -0.6380, -0.5763, -1.4406,  1.1512,  1.4887,  1.5425, -1.2619,\n",
              "          0.4954, -0.0567,  0.3455,  0.2214,  0.3082, -0.0330,  0.5192, -0.1269,\n",
              "         -0.8516, -0.9358, -0.2498,  2.3519, -1.2349,  1.2314,  1.0482,  0.8779,\n",
              "         -0.2674,  0.5843, -0.9939, -0.1741, -0.6176,  0.3609,  0.0234, -0.7035,\n",
              "          2.0671, -1.5142,  1.2433, -0.3402,  0.5209,  0.5139,  0.5245,  0.9005,\n",
              "         -0.3312,  0.0881,  0.1633,  1.4957, -1.0520, -0.1243,  1.5505,  1.0481,\n",
              "         -0.9548, -0.5288,  0.9242, -0.3488, -0.8274,  0.4534,  1.4184,  0.7917,\n",
              "          0.3727, -0.2051, -1.4472,  0.3576, -1.4009,  2.1265, -1.6407,  0.2618,\n",
              "         -0.2441,  0.2226, -0.7923,  0.0893, -0.1287, -0.2776, -0.7583, -0.2731,\n",
              "          0.6154,  0.4579, -0.5338, -1.3442, -1.7190, -1.2496,  0.7373, -1.2321,\n",
              "          0.3446, -0.3623,  0.7295,  0.3147, -0.7133,  2.1220,  1.1889,  0.9827,\n",
              "         -1.2590,  1.2363, -0.0371,  1.0349, -0.1459,  1.8347,  0.3373, -0.3517,\n",
              "          0.9582, -0.8802,  0.9181, -1.1254, -0.6867,  0.1645,  0.0292, -1.6531,\n",
              "         -0.3215,  1.1848,  0.9830,  0.0349, -0.1664, -0.5301,  0.5142, -0.1860,\n",
              "          0.9876,  1.0988, -0.7963, -0.3345, -0.6978, -1.1044, -0.0645,  0.1081,\n",
              "          0.2689,  0.2627, -0.3603, -0.1754,  1.4515, -1.2324, -0.9416, -0.6011,\n",
              "          0.7033,  1.2036, -1.0152,  0.1269,  2.2846, -0.0408], device='cuda:0'),\n",
              " StandardScaler(),\n",
              " StandardScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_size in subset_sizes:\n",
        "  train_x, test_x, train_y, test_y, _, _ = load_data(filepath=\"/content/drive/MyDrive/datasets/concrete.npz\", subset_size=subset_size)\n",
        "  model, likelihood, training_time = train_exactGP(train_x, train_y, epochs=10)\n",
        "  metrics = gp_eval(model, likelihood, test_x, test_y)\n",
        "  print(f\"Subset Size: {subset_size}\")\n",
        "  print(f\"Training Time: {training_time:.2f}s\")\n",
        "  print(f\"RMSE: {metrics[0]:.4f}\")\n",
        "  print(f\"NLL: {metrics[1]:.4f}\")\n",
        "  print(f\"Calibration: {metrics[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmTMmNwV9r4h",
        "outputId": "ad011e77-550d-4cc9-c729-28c55bdee216"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: X=(1030, 8), y=(1030,)\n",
            "Subset: X=(1000, 8), y=(1000,)\n",
            "Epoch: 10/10, Loss: 0.8289266228675842\n",
            "Training time - 0.09s\n",
            "Subset Size: 1000\n",
            "Training Time: 0.09s\n",
            "RMSE: 0.3928\n",
            "NLL: 123.9131\n",
            "Calibration: 1.0000\n",
            "Shape: X=(1030, 8), y=(1030,)\n",
            "Epoch: 10/10, Loss: 0.8227713704109192\n",
            "Training time - 0.21s\n",
            "Subset Size: 2000\n",
            "Training Time: 0.21s\n",
            "RMSE: 0.3816\n",
            "NLL: 129.3680\n",
            "Calibration: 1.0000\n",
            "Shape: X=(1030, 8), y=(1030,)\n",
            "Epoch: 10/10, Loss: 0.8217010498046875\n",
            "Training time - 0.19s\n",
            "Subset Size: 5000\n",
            "Training Time: 0.19s\n",
            "RMSE: 0.3814\n",
            "NLL: 129.4923\n",
            "Calibration: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PROTEIN\")\n",
        "load_data(filepath=\"/content/drive/MyDrive/datasets/protein.npz\", subset_size=None, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCbBzHMv-K1w",
        "outputId": "824a08ca-b850-4c2b-c184-ec184139e448"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROTEIN\n",
            "Shape: X=(45730, 9), y=(45730,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.1438, -0.6733,  1.3608,  ..., -0.7736, -0.5477,  1.3764],\n",
              "         [-0.5062, -0.6366, -0.5668,  ..., -0.0980, -0.3343,  0.2518],\n",
              "         [-0.3346, -0.2579,  0.1212,  ...,  0.1484, -0.7788,  0.1132],\n",
              "         ...,\n",
              "         [-0.3282, -0.6805, -1.0441,  ..., -0.3944, -0.8677,  0.3763],\n",
              "         [-0.8380, -0.2112,  1.8404,  ..., -0.8575, -1.1343,  1.3212],\n",
              "         [-0.5511, -0.6763, -0.5871,  ..., -0.3655, -0.9921,  0.5723]],\n",
              "        device='cuda:0'),\n",
              " tensor([[-0.7005, -0.3832,  0.7451,  ...,  0.0085, -1.1877,  0.7028],\n",
              "         [ 1.7822,  2.0305,  0.7629,  ...,  0.7980,  1.1590, -1.8750],\n",
              "         [ 1.5308,  1.9034,  0.9319,  ...,  0.8780,  1.4079, -1.7829],\n",
              "         ...,\n",
              "         [-0.3919, -0.4591, -0.3055,  ..., -0.2295,  0.4657,  0.3935],\n",
              "         [ 1.7062,  2.0048,  0.8294,  ...,  0.6012,  0.6434, -1.6183],\n",
              "         [-0.7031, -0.3198,  0.9634,  ...,  0.0495, -0.7610, -0.0187]],\n",
              "        device='cuda:0'),\n",
              " tensor([-0.5594, -0.7696,  1.0588,  ...,  1.0207, -0.1574, -0.9763],\n",
              "        device='cuda:0'),\n",
              " tensor([ 1.8164, -0.9680, -0.8211,  ..., -0.9279, -0.3242,  1.4384],\n",
              "        device='cuda:0'),\n",
              " StandardScaler(),\n",
              " StandardScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_size in subset_sizes:\n",
        "  train_x, test_x, train_y, test_y, _, _ = load_data(filepath=\"/content/drive/MyDrive/datasets/protein.npz\", subset_size=subset_size)\n",
        "  model, likelihood, training_time = train_exactGP(train_x, train_y, epochs=10)\n",
        "  metrics = gp_eval(model, likelihood, test_x, test_y)\n",
        "  print(f\"Subset Size: {subset_size}\")\n",
        "  print(f\"Training Time: {training_time:.2f}s\")\n",
        "  print(f\"RMSE: {metrics[0]:.4f}\")\n",
        "  print(f\"NLL: {metrics[1]:.4f}\")\n",
        "  print(f\"Calibration: {metrics[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWpMfAQT-vlk",
        "outputId": "d8bf6f4c-b1fc-45f2-d4d9-934f0faf3ee9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: X=(45730, 9), y=(45730,)\n",
            "Subset: X=(1000, 9), y=(1000,)\n",
            "Epoch: 10/10, Loss: 1.2664333581924438\n",
            "Training time - 0.09s\n",
            "Subset Size: 1000\n",
            "Training Time: 0.09s\n",
            "RMSE: 0.8040\n",
            "NLL: 234.2989\n",
            "Calibration: 0.9500\n",
            "Shape: X=(45730, 9), y=(45730,)\n",
            "Subset: X=(2000, 9), y=(2000,)\n",
            "Epoch: 10/10, Loss: 1.2102549076080322\n",
            "Training time - 0.20s\n",
            "Subset Size: 2000\n",
            "Training Time: 0.20s\n",
            "RMSE: 0.7495\n",
            "NLL: 450.0314\n",
            "Calibration: 0.9500\n",
            "Shape: X=(45730, 9), y=(45730,)\n",
            "Subset: X=(5000, 9), y=(5000,)\n",
            "Epoch: 10/10, Loss: 1.1579676866531372\n",
            "Training time - 0.47s\n",
            "Subset Size: 5000\n",
            "Training Time: 0.47s\n",
            "RMSE: 0.7021\n",
            "NLL: 1064.7396\n",
            "Calibration: 0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SARCOS\")\n",
        "load_data(filepath=\"/content/drive/MyDrive/datasets/sarcos.npz\", subset_size=None, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO6Y9A0W-9tf",
        "outputId": "cc2411c2-9ec6-4120-9507-ec6ba4d7f263"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARCOS\n",
            "Shape: X=(48933, 21), y=(48933,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.5099,  1.0572, -0.7961,  ...,  1.4406,  0.9679, -1.6581],\n",
              "         [-0.8785, -0.3972, -2.3847,  ...,  1.9720,  2.5982, -0.4899],\n",
              "         [-1.4277, -0.9678,  1.0731,  ..., -0.4240,  0.2015,  0.1482],\n",
              "         ...,\n",
              "         [-0.3338, -0.4206, -0.2062,  ..., -1.3949,  1.1409,  1.5486],\n",
              "         [-1.1535, -0.7938,  0.5435,  ..., -1.9183, -1.5790,  1.3357],\n",
              "         [-0.5671,  0.0440, -1.7767,  ...,  0.0747,  0.2436, -1.2036]],\n",
              "        device='cuda:0'),\n",
              " tensor([[ 0.5365,  0.7983, -0.3437,  ...,  0.3153, -1.0161, -1.6491],\n",
              "         [ 0.5811,  1.0840,  1.6059,  ..., -0.6889,  0.2703, -0.7412],\n",
              "         [ 0.5637,  0.8786, -0.3662,  ...,  0.0714,  0.2861, -0.0488],\n",
              "         ...,\n",
              "         [ 0.8491,  1.0984,  0.9888,  ...,  0.3438, -0.5824,  0.7398],\n",
              "         [ 2.4173,  0.6410,  0.5246,  ..., -0.1668,  0.0556,  0.4851],\n",
              "         [-0.3071,  0.9642, -0.7939,  ...,  1.4792, -1.5855, -1.4379]],\n",
              "        device='cuda:0'),\n",
              " tensor([ 1.3420,  2.7313, -0.6252,  ..., -0.3906, -0.9300, -0.3973],\n",
              "        device='cuda:0'),\n",
              " tensor([ 0.7163,  0.4114,  0.1728,  ..., -0.2302, -1.5241,  0.3849],\n",
              "        device='cuda:0'),\n",
              " StandardScaler(),\n",
              " StandardScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_size in subset_sizes:\n",
        "  train_x, test_x, train_y, test_y, _, _ = load_data(filepath=\"/content/drive/MyDrive/datasets/sarcos.npz\", subset_size=subset_size)\n",
        "  model, likelihood, training_time = train_exactGP(train_x, train_y, epochs=10)\n",
        "  metrics = gp_eval(model, likelihood, test_x, test_y)\n",
        "  print(f\"Subset Size: {subset_size}\")\n",
        "  print(f\"Training Time: {training_time:.2f}s\")\n",
        "  print(f\"RMSE: {metrics[0]:.4f}\")\n",
        "  print(f\"NLL: {metrics[1]:.4f}\")\n",
        "  print(f\"Calibration: {metrics[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZJvOAP4_XFs",
        "outputId": "6c94a443-62ee-4901-c678-98eb0a92f16c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: X=(48933, 21), y=(48933,)\n",
            "Subset: X=(1000, 21), y=(1000,)\n",
            "Epoch: 10/10, Loss: 1.187052845954895\n",
            "Training time - 0.09s\n",
            "Subset Size: 1000\n",
            "Training Time: 0.09s\n",
            "RMSE: 0.7095\n",
            "NLL: 195.7430\n",
            "Calibration: 0.9700\n",
            "Shape: X=(48933, 21), y=(48933,)\n",
            "Subset: X=(2000, 21), y=(2000,)\n",
            "Epoch: 10/10, Loss: 1.0731534957885742\n",
            "Training time - 0.19s\n",
            "Subset Size: 2000\n",
            "Training Time: 0.19s\n",
            "RMSE: 0.6448\n",
            "NLL: 375.7748\n",
            "Calibration: 0.9725\n",
            "Shape: X=(48933, 21), y=(48933,)\n",
            "Subset: X=(5000, 21), y=(5000,)\n",
            "Epoch: 10/10, Loss: 0.9254888892173767\n",
            "Training time - 0.51s\n",
            "Subset Size: 5000\n",
            "Training Time: 0.51s\n",
            "RMSE: 0.4189\n",
            "NLL: 759.9874\n",
            "Calibration: 0.9910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SYNTHETIC\")\n",
        "load_data(filepath=\"/content/drive/MyDrive/datasets/synthetic_heteroscedastic.npz\", subset_size=None, test_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVrzYuEt_d7t",
        "outputId": "a76168b2-c630-49a3-bb27-37d8434b4538"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYNTHETIC\n",
            "Shape: X=(5000, 2), y=(5000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.8327, -0.2844],\n",
              "         [-0.2644,  0.9771],\n",
              "         [ 0.3035, -0.4082],\n",
              "         ...,\n",
              "         [-1.5917,  0.9212],\n",
              "         [ 0.8969,  0.5512],\n",
              "         [-0.7564, -0.4181]], device='cuda:0'),\n",
              " tensor([[ 0.5876, -1.0854],\n",
              "         [-0.0192,  0.0443],\n",
              "         [ 0.4450,  0.7949],\n",
              "         ...,\n",
              "         [ 0.4648,  0.2451],\n",
              "         [ 0.5965, -0.9930],\n",
              "         [-0.5957,  0.3378]], device='cuda:0'),\n",
              " tensor([-0.4159, -1.2998,  1.4912,  ...,  0.6883,  0.6907, -0.6203],\n",
              "        device='cuda:0'),\n",
              " tensor([ 1.0285e+00,  7.5425e-01,  8.9678e-01,  1.7980e+00, -1.9474e-02,\n",
              "         -6.1743e-01,  3.1872e-01, -1.4240e+00, -1.5981e+00, -8.3311e-01,\n",
              "         -2.1695e-01, -1.6458e+00, -1.3591e-02, -1.9082e+00,  4.0394e-02,\n",
              "          6.1075e-01, -1.5826e+00,  5.3169e-01,  9.5945e-02,  1.6668e+00,\n",
              "         -1.7635e-01, -4.6177e-01,  1.0586e+00, -1.2007e-01,  5.9591e-01,\n",
              "          1.0696e+00, -5.2949e-02,  5.9446e-01,  5.9145e-01,  3.5968e-01,\n",
              "         -8.0533e-01,  3.4676e-01, -1.1119e+00,  6.6846e-01, -6.6303e-01,\n",
              "          3.0289e-01,  1.5338e+00,  8.9955e-01,  7.7032e-01, -1.2300e+00,\n",
              "         -7.3882e-01,  4.8518e-01,  1.7322e-01,  1.0605e+00,  1.7659e+00,\n",
              "         -4.3975e-01, -4.6821e-01,  2.7940e-01, -5.4457e-01, -1.2292e+00,\n",
              "          1.2028e+00,  1.6315e+00,  6.0721e-01,  1.4430e+00, -1.1449e+00,\n",
              "          3.0005e-01, -1.0525e+00, -8.7621e-01, -1.4242e+00,  9.5031e-01,\n",
              "         -1.5739e+00,  5.0219e-01, -6.5947e-01, -7.1643e-01, -1.2655e+00,\n",
              "          7.4032e-01,  1.7764e+00, -7.0912e-01,  1.7168e+00,  5.5352e-01,\n",
              "          4.1341e-01,  1.0329e+00,  2.1638e-01,  1.3049e+00, -1.1372e-01,\n",
              "          5.4076e-01, -6.2618e-01, -1.9307e-01,  9.2504e-01,  2.5704e-01,\n",
              "         -4.7633e-01,  7.5818e-02,  8.9162e-01,  9.6311e-01, -2.0500e-01,\n",
              "          2.5866e-01, -6.9521e-01,  6.2627e-01,  6.1397e-01,  1.3033e+00,\n",
              "         -7.0783e-01,  8.6584e-01, -1.1690e+00,  5.2775e-02, -1.2554e+00,\n",
              "         -6.0325e-02,  5.4302e-01,  1.1712e+00,  5.0597e-02, -6.5739e-01,\n",
              "         -4.8520e-01,  2.6500e-01,  1.8700e+00, -1.7976e+00,  6.1579e-01,\n",
              "          1.7109e+00, -1.1047e+00, -6.6294e-01, -7.3017e-01, -6.8939e-01,\n",
              "          1.7119e+00, -8.3683e-01, -1.3651e-01,  1.0374e+00,  1.2028e+00,\n",
              "          2.4768e-01,  2.9893e-01, -1.3393e-01, -9.8169e-01,  2.1019e+00,\n",
              "          1.3204e+00, -1.5906e+00, -2.0710e+00, -5.7292e-01, -2.7144e-01,\n",
              "         -1.5713e-01,  1.1390e+00,  7.4614e-01,  1.8333e-01, -1.2365e+00,\n",
              "         -1.7969e+00,  1.3433e+00,  7.0246e-01,  3.6739e-01,  1.8021e+00,\n",
              "         -6.5735e-01,  4.0288e-02, -1.1437e+00,  1.4644e+00, -2.5601e-01,\n",
              "          1.4692e-02, -2.7152e-01, -1.4454e+00, -1.3563e+00, -1.0508e+00,\n",
              "          2.1614e-01,  1.0158e+00, -9.8137e-01, -8.7544e-01, -4.5539e-01,\n",
              "          1.1356e+00, -5.9295e-01,  1.4410e-01, -1.1233e+00,  1.6169e+00,\n",
              "          7.5274e-01, -1.4713e+00,  6.6522e-01, -1.4777e+00,  1.0290e+00,\n",
              "         -2.5002e-01, -1.0871e+00,  1.3517e+00, -6.6180e-01, -3.9559e-01,\n",
              "         -9.7254e-01,  1.1402e+00, -1.3012e+00,  3.2357e-01, -8.1510e-01,\n",
              "         -1.2681e+00, -1.5135e+00, -1.3566e+00,  1.2203e+00, -1.4392e+00,\n",
              "          3.3460e-01,  1.1571e+00,  1.2860e+00, -1.1229e+00,  5.8253e-01,\n",
              "         -4.8340e-01,  7.4441e-01, -1.1400e+00, -1.2508e+00, -1.7062e+00,\n",
              "          6.3940e-01,  1.6987e+00, -1.3532e+00, -1.5568e-01,  1.4411e+00,\n",
              "          1.0406e+00,  6.9304e-01,  1.1036e+00, -2.5974e-01, -1.2155e+00,\n",
              "         -7.6435e-02, -3.3844e-02,  9.3400e-01,  1.1447e+00, -1.4933e-02,\n",
              "         -1.0819e-01, -4.3890e-01,  1.2610e+00, -3.8241e-01, -9.3542e-01,\n",
              "          1.5077e-01, -1.3287e+00, -9.3809e-01, -1.1441e+00,  6.4161e-01,\n",
              "         -2.6018e-01, -4.1611e-01, -9.1160e-01, -2.5244e-02,  1.8433e+00,\n",
              "         -1.5612e+00,  1.7831e+00,  6.7858e-01,  1.0070e+00,  9.0401e-01,\n",
              "          2.7151e-01,  4.4966e-01, -1.3120e+00, -6.9689e-01, -9.1174e-01,\n",
              "          7.6352e-02, -4.5110e-01,  1.0605e+00,  8.9592e-01,  9.5963e-01,\n",
              "          6.1468e-02, -7.0492e-01,  3.5650e-01, -8.6340e-01,  5.6803e-01,\n",
              "          2.2366e-01,  6.5475e-01, -1.0378e+00,  4.1666e-01, -6.0695e-02,\n",
              "          1.2760e-01, -9.3036e-01, -1.4380e+00,  1.3251e+00, -5.0404e-01,\n",
              "         -8.9364e-01,  1.4994e+00, -6.8423e-01, -1.6748e-01,  1.1841e+00,\n",
              "          4.4901e-01,  1.0035e+00,  5.6793e-01,  9.5269e-01,  4.3005e-01,\n",
              "         -6.4345e-01, -1.2986e+00, -2.4263e-01,  7.3584e-01, -1.4989e+00,\n",
              "         -4.2883e-01,  5.2587e-01, -7.5288e-01,  3.6971e-01, -1.8360e-01,\n",
              "         -5.7944e-01, -1.4007e-01, -1.6620e+00, -1.4850e+00,  5.5310e-01,\n",
              "         -1.1731e+00,  8.1303e-01, -3.3387e-01, -1.2895e+00,  7.9259e-02,\n",
              "          8.2335e-01,  4.8367e-01,  4.2213e-01, -1.8536e+00, -3.8771e-02,\n",
              "          1.8139e+00, -7.0656e-01,  3.1916e-01, -1.2899e+00, -9.2616e-01,\n",
              "          2.7592e-02,  7.7273e-01,  6.5769e-01, -9.6100e-01, -6.6589e-01,\n",
              "          1.0522e+00, -9.6046e-01, -1.6862e-01,  8.3315e-01,  1.9057e+00,\n",
              "          7.6093e-01, -9.1088e-01, -7.7517e-01, -1.4901e+00, -2.9466e-01,\n",
              "          6.5953e-01,  7.0032e-01,  6.3456e-01, -6.6073e-01, -3.2249e-01,\n",
              "          1.3372e+00,  1.2226e-01, -8.1362e-01, -6.2555e-01, -1.3251e+00,\n",
              "          3.4330e-02, -9.8469e-01,  3.5734e-01,  3.0613e-01, -7.3846e-01,\n",
              "          1.1661e+00, -1.1732e+00, -8.9308e-01,  1.1109e+00,  1.8190e-01,\n",
              "          7.0036e-01,  7.1174e-01,  1.8149e+00, -1.1599e+00,  1.2883e+00,\n",
              "         -6.2262e-01, -5.2977e-02,  3.6363e-01, -1.5417e+00, -1.4082e+00,\n",
              "         -9.5637e-01, -8.2025e-01,  8.0089e-01,  7.4177e-03,  8.9648e-01,\n",
              "         -2.6671e-01,  3.2682e-01, -1.6900e+00,  9.3692e-01, -8.4962e-01,\n",
              "         -1.7559e+00,  9.4062e-01,  7.1859e-01,  6.9516e-01, -1.5562e-01,\n",
              "         -9.3382e-01,  1.0198e+00,  9.1061e-01, -1.1829e-01,  1.8814e+00,\n",
              "          2.0750e-02, -1.3011e+00,  5.8561e-01,  1.1149e+00, -8.2020e-01,\n",
              "         -4.3061e-01,  2.0939e-01,  8.8852e-01, -2.0253e-01,  1.1638e+00,\n",
              "          1.0459e+00, -5.5517e-02, -8.8390e-01, -8.5062e-01,  1.4251e+00,\n",
              "         -3.9947e-01,  1.2345e+00,  7.6677e-01, -7.8334e-01, -1.4947e+00,\n",
              "         -1.7483e+00, -9.9995e-02, -3.4717e-01, -1.4583e+00,  9.2148e-01,\n",
              "         -6.2461e-01,  9.5496e-02, -3.6733e-01, -1.2375e-01,  1.3149e+00,\n",
              "          1.8155e+00, -9.0102e-01, -1.6228e+00,  6.2441e-01,  1.0340e+00,\n",
              "         -2.1007e-01, -6.2658e-02,  1.5811e+00, -3.6314e-01,  1.5044e+00,\n",
              "         -1.8929e-01, -1.0907e+00,  4.7175e-01,  2.6155e-01, -9.3106e-01,\n",
              "          6.0866e-01,  1.7811e+00, -8.7402e-01, -1.8565e+00, -1.0128e+00,\n",
              "          1.7381e+00,  1.1435e+00,  1.0698e+00, -4.0660e-01,  1.1274e+00,\n",
              "         -7.1851e-01, -6.2793e-01, -1.4816e+00, -1.1023e+00,  3.9539e-01,\n",
              "         -7.0381e-01, -2.3533e-01, -5.1376e-01, -1.2547e+00,  5.8609e-02,\n",
              "         -9.1338e-01, -1.6503e+00, -1.6037e-01,  1.8445e+00,  6.2555e-01,\n",
              "          1.3325e+00,  7.7664e-03,  3.0702e-01, -1.9709e+00,  2.0413e-01,\n",
              "         -1.2123e+00,  9.1691e-02, -1.5779e+00,  1.3454e+00,  1.0117e+00,\n",
              "          1.2217e+00,  2.9939e-01,  1.5799e+00, -8.5572e-01, -1.1335e+00,\n",
              "          8.9598e-01,  7.2777e-01, -7.7165e-01,  1.7542e+00, -9.7386e-01,\n",
              "         -3.1197e-02, -8.2573e-01, -4.7599e-01, -1.2262e-01,  1.6343e+00,\n",
              "         -1.4875e+00,  2.0297e-01,  2.2509e-01, -6.6981e-01,  7.0852e-01,\n",
              "         -6.5947e-01,  7.5575e-01, -1.5482e+00, -1.3628e+00,  1.1934e+00,\n",
              "          5.7632e-02, -5.7146e-01, -4.6559e-01, -5.6426e-01, -3.5987e-01,\n",
              "         -1.3508e+00, -1.0892e+00, -2.4678e-01, -1.1165e+00, -9.6343e-01,\n",
              "          7.7519e-01, -5.1311e-01,  1.4269e+00, -6.9876e-01, -1.5253e+00,\n",
              "          6.9341e-01,  1.6756e+00,  1.6089e+00,  2.7510e-01,  8.5669e-02,\n",
              "         -4.9231e-01, -3.8787e-01, -1.5410e+00, -3.7436e-01, -4.2248e-01,\n",
              "          3.4367e-01,  1.2761e+00,  8.6124e-01,  6.1656e-01, -8.6927e-01,\n",
              "         -1.9433e+00,  1.2734e+00, -5.3652e-01, -5.9613e-01,  2.7666e-01,\n",
              "          9.5321e-02,  1.2642e-01,  5.1928e-01,  4.2179e-01,  6.2255e-01,\n",
              "          2.1257e-01, -1.1510e+00,  1.6046e+00,  9.4486e-01,  1.6253e+00,\n",
              "          1.8264e+00,  6.9818e-01,  3.1446e-01,  2.2924e-01,  1.8427e+00,\n",
              "          5.8035e-02, -2.9880e-01,  3.9439e-01,  2.0843e+00,  1.2682e-01,\n",
              "          6.8292e-01, -1.1074e+00,  2.3094e-02, -4.8595e-01,  1.1487e+00,\n",
              "          1.1935e+00, -9.6191e-01, -7.6039e-01,  7.6009e-01, -1.3829e+00,\n",
              "          9.5967e-01, -7.4581e-01,  1.5163e+00, -1.7330e+00,  2.9426e-01,\n",
              "         -1.5350e+00,  1.6751e-01,  1.3294e+00,  5.6435e-02,  1.7104e+00,\n",
              "         -4.6792e-01,  1.0582e+00,  1.2932e+00,  5.3109e-01,  5.3491e-02,\n",
              "          1.7696e+00, -1.6781e+00, -1.3725e+00,  2.0091e-01, -4.6448e-01,\n",
              "         -1.3630e-01,  1.4414e+00,  9.3536e-01, -8.4720e-01,  9.1774e-01,\n",
              "         -2.0983e-01, -1.2789e+00, -2.9991e-01,  1.4169e+00, -6.9255e-01,\n",
              "          8.9869e-02, -4.5382e-01,  3.2439e-01, -2.7031e-01, -1.6962e+00,\n",
              "          7.3479e-01, -6.7446e-01, -8.6373e-01, -1.4625e+00, -5.6320e-01,\n",
              "          1.5324e+00, -7.7142e-01, -2.6595e-01, -8.7779e-01,  8.8874e-01,\n",
              "          6.2719e-01, -5.9551e-01, -8.7728e-01, -1.1606e+00,  1.0680e+00,\n",
              "          1.0344e+00,  8.1507e-01,  6.4115e-01,  7.6915e-01,  4.1915e-03,\n",
              "         -8.7252e-01, -1.2658e+00,  2.5861e-01,  4.4044e-02,  1.3686e+00,\n",
              "          9.1051e-01, -1.8300e+00, -1.6762e+00, -6.0617e-01,  1.1729e-01,\n",
              "          1.0809e+00,  1.4287e+00, -1.4355e+00, -1.3185e+00,  1.0539e+00,\n",
              "          1.5090e+00, -1.1268e+00, -7.7693e-01, -1.6109e+00,  1.5902e+00,\n",
              "          1.5495e+00, -7.9650e-01,  5.2267e-01, -1.3093e+00, -7.5196e-01,\n",
              "          1.4267e+00, -7.9251e-01, -5.2290e-01, -5.6150e-01,  1.6362e+00,\n",
              "         -9.3777e-01,  7.9680e-01, -1.8990e-02,  4.7969e-01,  9.7851e-01,\n",
              "         -1.0575e+00, -1.3786e+00, -1.1991e+00, -3.8881e-02, -9.6138e-01,\n",
              "          4.0017e-01, -2.5634e-01, -5.0939e-01,  7.1109e-01, -8.4813e-01,\n",
              "          1.9860e+00, -2.4547e-01,  8.1576e-01, -3.3964e-01,  6.8517e-01,\n",
              "         -9.1509e-01,  5.2875e-01, -3.9715e-01,  8.4037e-01, -1.3213e+00,\n",
              "         -6.3912e-01,  9.4285e-01,  1.9523e+00, -1.0159e+00,  9.4923e-01,\n",
              "          7.6032e-01,  8.5532e-01, -7.3577e-01,  1.5325e+00, -8.1252e-01,\n",
              "         -6.7858e-01,  7.8013e-01, -2.7674e-01, -1.2569e-01, -9.2483e-01,\n",
              "          1.6383e+00,  1.1366e+00, -1.1602e+00, -6.2004e-01, -1.6592e+00,\n",
              "          9.4214e-01,  1.3135e+00, -1.3092e+00,  1.9928e+00, -9.3827e-01,\n",
              "          1.1195e+00, -1.0032e+00, -6.5080e-01,  2.6095e-01, -1.3032e+00,\n",
              "         -7.1158e-01, -8.2135e-01, -1.2094e+00, -1.2392e+00, -1.5297e+00,\n",
              "         -4.4089e-01, -2.5085e-01, -2.9328e-01,  1.9742e+00,  1.1018e+00,\n",
              "          2.0316e-01,  3.2452e-01, -1.4331e+00, -1.1044e+00,  1.0250e+00,\n",
              "          6.0164e-03, -7.7768e-01, -1.0987e+00, -4.6253e-01, -1.6624e+00,\n",
              "          1.1923e+00,  7.9254e-01, -2.2107e-01,  2.0159e-01, -1.6261e+00,\n",
              "          2.7751e-01, -5.7131e-01, -1.9775e+00,  3.2702e-01, -2.6857e-01,\n",
              "         -4.4800e-01,  5.3077e-01,  6.7514e-01, -1.0195e+00,  6.9305e-01,\n",
              "          4.5813e-01,  6.9261e-01,  5.7068e-01,  1.3474e+00, -1.0913e+00,\n",
              "         -6.1230e-01, -1.4081e+00, -1.3058e+00,  4.5714e-01,  2.2643e-01,\n",
              "          1.4646e+00, -1.5388e-01,  1.9699e+00,  1.4019e-01, -1.5211e+00,\n",
              "         -1.2307e+00,  3.7078e-01, -2.8448e-01,  6.3255e-01, -9.4027e-01,\n",
              "          3.5929e-01,  1.6374e+00,  8.1290e-01,  1.1713e+00,  2.2465e-01,\n",
              "         -3.3401e-01, -1.2969e+00, -3.1511e-01, -1.1175e+00,  7.4176e-01,\n",
              "          1.5772e+00,  1.3649e+00,  1.1133e+00, -4.6459e-01, -1.0416e+00,\n",
              "         -1.4096e+00,  2.7881e-02, -2.5852e-01,  1.7423e-01,  4.3973e-01,\n",
              "         -8.4743e-01, -1.2622e+00, -4.4674e-02,  2.3742e-01,  8.8763e-01,\n",
              "         -1.2121e+00,  8.0921e-01,  1.2209e+00, -2.4016e-01, -1.7533e+00,\n",
              "         -6.7219e-01, -1.6849e+00,  1.3085e+00,  3.2849e-01, -1.7086e+00,\n",
              "          2.0024e+00, -1.0838e+00, -1.2909e+00,  8.3484e-02,  3.0733e-01,\n",
              "         -4.7875e-01, -1.9137e-01, -1.6002e+00,  1.0265e+00, -7.2867e-01,\n",
              "          8.1931e-01,  5.5245e-01, -1.0816e+00, -8.7551e-01, -7.0744e-01,\n",
              "          1.0315e+00, -4.9252e-01, -8.3923e-01, -8.6763e-01, -1.0641e-01,\n",
              "         -1.0489e+00,  2.0959e-02, -9.8176e-01, -1.1885e+00,  1.2746e+00,\n",
              "          4.2601e-01, -1.5543e+00,  1.0714e+00, -7.8151e-01,  8.2154e-01,\n",
              "          1.4662e+00, -8.0408e-02,  3.0636e-01,  5.1212e-01,  4.3307e-01,\n",
              "          1.0558e+00, -1.3536e+00, -3.5132e-01,  1.5584e+00,  5.9659e-01,\n",
              "          1.8826e+00, -5.6881e-02,  1.5015e+00,  8.7649e-01,  9.1804e-01,\n",
              "          6.5317e-02,  9.9909e-01, -7.2775e-01, -1.7377e+00, -1.0329e+00,\n",
              "         -6.9955e-01, -9.0991e-02, -1.0384e+00,  1.2215e+00, -6.3751e-01,\n",
              "         -7.8044e-01,  1.0948e+00, -2.0683e-01,  6.3265e-01,  1.2042e+00,\n",
              "         -1.6239e-01, -4.7055e-01,  7.6202e-01,  1.8507e+00,  1.2078e+00,\n",
              "         -1.6677e+00,  7.5632e-01, -1.1724e+00, -3.0187e-01, -1.1507e+00,\n",
              "          1.5900e+00,  8.0194e-01,  4.8707e-01, -4.7634e-01, -1.3376e+00,\n",
              "          1.2906e+00, -9.0029e-01,  1.0939e+00,  1.6844e+00, -1.2535e+00,\n",
              "         -1.1147e+00, -2.2007e-01, -5.5351e-01,  1.0383e-01, -9.7225e-01,\n",
              "          9.2655e-01,  1.5336e-01,  8.2940e-01,  1.2778e+00,  8.1937e-01,\n",
              "         -3.3170e-01, -1.7223e+00, -9.7927e-02,  5.9110e-01,  1.1486e+00,\n",
              "          7.6549e-01, -2.1351e-01,  1.7152e+00,  1.1938e+00, -1.6700e-03,\n",
              "          1.0662e+00, -1.5448e+00,  1.8583e+00, -2.0303e-01,  1.5968e+00,\n",
              "         -7.9328e-01,  1.3670e+00, -3.8882e-01, -4.6694e-01, -7.1828e-01,\n",
              "          8.7700e-01, -3.6168e-01,  1.9217e-01,  7.1448e-01, -2.3243e-01,\n",
              "         -6.2681e-01, -4.7560e-01, -3.5608e-01, -7.4918e-01, -1.1834e+00,\n",
              "         -1.8300e-01,  2.3932e-01,  2.3875e-01, -1.0319e+00, -7.5211e-01,\n",
              "         -9.2935e-01,  5.6201e-01,  1.0648e-01, -1.3535e+00,  9.5515e-01,\n",
              "         -1.1994e+00,  2.0803e+00, -1.4574e+00,  1.0520e+00, -4.7891e-01,\n",
              "         -1.1185e+00, -1.4421e-02, -6.1214e-01,  1.3759e+00,  1.7891e-01,\n",
              "         -8.1327e-01, -1.0075e+00, -1.4749e+00,  1.5634e+00,  7.0757e-01,\n",
              "         -1.8241e+00, -1.0180e+00,  1.8330e+00,  9.7300e-01,  1.1579e-01,\n",
              "          9.9826e-01,  6.7787e-01, -1.0846e+00,  4.5064e-01,  9.4108e-01,\n",
              "          5.3427e-01,  1.5863e+00,  6.1812e-01, -8.0795e-01,  1.6756e+00,\n",
              "          1.1980e+00,  1.6736e+00, -1.1786e+00, -1.8090e-01, -1.4620e+00,\n",
              "         -6.3608e-01,  1.7867e+00, -1.2344e+00, -2.1866e-01,  1.7200e+00,\n",
              "          9.5081e-01,  2.5717e-01,  4.3682e-01, -2.8889e-01, -1.2933e-01,\n",
              "          9.0457e-01, -8.3308e-01, -8.6301e-01,  8.7369e-01, -2.5164e-01,\n",
              "          1.2567e+00, -4.0190e-01,  1.4357e+00, -3.6346e-01, -1.1510e+00,\n",
              "         -1.5457e-02,  5.4433e-01,  9.0968e-01, -1.6944e+00,  5.2341e-01,\n",
              "         -2.7064e-01, -6.8331e-01,  4.5202e-01,  8.2370e-01, -1.3991e+00,\n",
              "         -8.3120e-02,  7.1534e-01, -9.4502e-01, -3.4100e-01, -1.1533e+00,\n",
              "          9.5057e-01, -7.5069e-01,  1.7966e+00,  8.8843e-01, -6.9104e-01,\n",
              "         -1.6316e+00,  8.2513e-01,  4.1309e-01, -9.7570e-01,  2.4899e-01,\n",
              "          3.7250e-01,  9.2802e-01, -8.3092e-01,  1.2715e+00, -7.9612e-01,\n",
              "          1.5260e+00,  8.1142e-01,  3.4521e-01,  9.8234e-01, -1.1499e-01,\n",
              "          7.9004e-01,  2.1642e-01,  7.1996e-01, -1.5578e+00,  1.0039e+00,\n",
              "          1.1366e+00,  1.0906e-01, -8.2320e-01, -1.3470e-01, -3.2802e-01,\n",
              "          1.7050e+00, -9.0696e-01,  9.1743e-01,  6.1818e-02,  4.8167e-01,\n",
              "         -7.5485e-01,  6.4447e-01, -8.6833e-02, -1.2835e+00, -8.4782e-01,\n",
              "         -1.4142e+00, -1.1956e+00,  5.4466e-01, -9.3433e-01, -8.6344e-01,\n",
              "         -8.6036e-01,  1.4427e+00,  1.8560e+00,  7.6404e-01, -6.7908e-01],\n",
              "        device='cuda:0'),\n",
              " StandardScaler(),\n",
              " StandardScaler())"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for subset_size in subset_sizes:\n",
        "  train_x, test_x, train_y, test_y, _, _ = load_data(filepath=\"/content/drive/MyDrive/datasets/synthetic_heteroscedastic.npz\", subset_size=subset_size)\n",
        "  model, likelihood, training_time = train_exactGP(train_x, train_y, epochs=10)\n",
        "  metrics = gp_eval(model, likelihood, test_x, test_y)\n",
        "  print(f\"Subset Size: {subset_size}\")\n",
        "  print(f\"Training Time: {training_time:.2f}s\")\n",
        "  print(f\"RMSE: {metrics[0]:.4f}\")\n",
        "  print(f\"NLL: {metrics[1]:.4f}\")\n",
        "  print(f\"Calibration: {metrics[2]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2mSym74_3u_",
        "outputId": "65641538-0fb3-4e3b-f2f5-d10f41250237"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: X=(5000, 2), y=(5000,)\n",
            "Subset: X=(1000, 2), y=(1000,)\n",
            "Epoch: 10/10, Loss: 0.512779176235199\n",
            "Training time - 0.09s\n",
            "Subset Size: 1000\n",
            "Training Time: 0.09s\n",
            "RMSE: 0.1765\n",
            "NLL: 81.0665\n",
            "Calibration: 1.0000\n",
            "Shape: X=(5000, 2), y=(5000,)\n",
            "Subset: X=(2000, 2), y=(2000,)\n",
            "Epoch: 10/10, Loss: 0.47150835394859314\n",
            "Training time - 0.23s\n",
            "Subset Size: 2000\n",
            "Training Time: 0.23s\n",
            "RMSE: 0.1910\n",
            "NLL: 161.5568\n",
            "Calibration: 1.0000\n",
            "Shape: X=(5000, 2), y=(5000,)\n",
            "Epoch: 10/10, Loss: 0.4511450231075287\n",
            "Training time - 0.48s\n",
            "Subset Size: 5000\n",
            "Training Time: 0.48s\n",
            "RMSE: 0.1637\n",
            "NLL: 383.9438\n",
            "Calibration: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RFF(torch.nn.Module):\n",
        "  def __init__(self, num_features=1000, lengthscale=1.0, sigma_noise=0.1):\n",
        "    super().__init__()\n",
        "    self.num_features = num_features\n",
        "    self.sigma_noise = sigma_noise\n",
        "    self.lengthscale = lengthscale\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "    self.linear_weights = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    D = X.shape[1]\n",
        "    self.weights = torch.randn(D, self.num_features, device=X.device) / self.lengthscale\n",
        "    self.bias = torch.randn(self.num_features, device=X.device) * 2 * np.pi\n",
        "\n",
        "    Z = self.transform(X)\n",
        "\n",
        "    jitter = self.sigma_noise**2 * torch.eye(self.num_features, device=X.device)\n",
        "    self.linear_weights = torch.linalg.solve(Z.T @ Z + jitter, Z.T @ y)\n",
        "\n",
        "  def transform(self, X):\n",
        "    projection = X @ self.weights + self.bias\n",
        "    Z = np.sqrt(2.0/self.num_features) * torch.cos(projection)\n",
        "    return Z\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    Z_star = self.transform(X_test)\n",
        "    pred_mean = Z_star @ self.linear_weights\n",
        "\n",
        "    return pred_mean, None"
      ],
      "metadata": {
        "id": "_BlDTeCAScJ5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Nystrom(torch.nn.Module):\n",
        "  def __init__(self, num_lamdmarks=500, lengthscale=1.0, sigma_noise=0.1):\n",
        "    super().__init__()\n",
        "    self.num_lamdmarks = num_lamdmarks\n",
        "    self.lengthscale = lengthscale\n",
        "    self.sigma_noise = sigma_noise\n",
        "    self.landmarks = None\n",
        "    self.K_mm_inv = None\n",
        "    self.alpha = None\n",
        "\n",
        "  def kernel(self, x1, x2):\n",
        "    dist = torch.cdist(x1, x2, p=2)\n",
        "    return torch.exp(-0.5 * (dist/self.lengthscale) ** 2)\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    mins = X.min(dim=0)[0]\n",
        "    maxs = X.max(dim=0)[0]\n",
        "    self.landmarks = torch.rand(self.num_lamdmarks, X.shape[1], device=X.device) * (maxs - mins) + mins\n",
        "\n",
        "    K_mm = self.kernel(self.landmarks, self.landmarks)\n",
        "    K_nm = self.kernel(X, self.landmarks)\n",
        "\n",
        "    K_mm_stable = K_mm + 1e-6 * torch.eye(self.num_lamdmarks, device=X.device)\n",
        "    L_mm = torch.linalg.cholesky(K_mm_stable)\n",
        "    Lambda = K_mm_stable + (1.0/self.sigma_noise**2) * (K_nm.t() @ K_nm)\n",
        "\n",
        "    self.woodbury_inv = torch.linalg.inv(Lambda)\n",
        "    self.weights = (1.0/self.sigma_noise**2) * (self.woodbury_inv @ K_nm.t() @ y)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    K_tm = self.kernel(X_test, self.landmarks)\n",
        "    mean = K_tm @ self.weights\n",
        "    return mean, None"
      ],
      "metadata": {
        "id": "fgzYhtZCYDMw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(X_full, y_full, subset_sizes=[1000, 2000, 5000]):\n",
        "  outputs = []\n",
        "  lengthscale = 1.0\n",
        "  noise = 0.2\n",
        "\n",
        "  for subset_size in subset_sizes:\n",
        "    X = X_full[:subset_size].to(device)\n",
        "    y = y_full[:subset_size].to(device)\n",
        "    X_test = X_full[-100].to(device)\n",
        "    y_test = y_full[-100].to(device)\n",
        "\n",
        "    #ExactGP\n",
        "    torch.cuda.empty_cache()\n",
        "    t0 = time.time()\n",
        "    model, likelihood, t_train = train_exactGP(X, y, epochs=10)\n",
        "    t_eval = time.time() - t0\n",
        "\n",
        "    metrics = gp_eval(model, likelihood, X_test, y_test)\n",
        "    outputs.append({\"method\" : \"exact GP\", \"N\" : subset_size, \"Time\": t_eval, \"RMSE\": metrics[\"RMSE\"]})\n",
        "\n",
        "    #RFF\n",
        "    torch.cuda.empty_cache()\n",
        "    t0 = time.time()\n",
        "    rff = RFF(num_features=1000, lengthscale=lengthscale, sigma_noise=noise).to(device)\n",
        "    rff.fit(X, y)\n",
        "    t_eval_rff = time.time() - t0\n",
        "\n",
        "    pred_mean, _ = rff.predict(X_test)\n",
        "    rmse_rff = torch.sqrt(torch.mean((pred_mean - y_test)**2)).item()\n",
        "    outputs.append({\"method\" : \"RFF\", \"N\" : subset_size, \"Time\": t_eval_r, \"RMSE\": rmse_rff})\n",
        "\n",
        "    #Nystrom\n",
        "    torch.cuda.empty_cache()\n",
        "    t0 = time.time()\n",
        "    nystrom = Nystrom(num_lamdmarks=500, lengthscale=lengthscale, sigma_noise=noise).to(device)\n",
        "    nystrom.fit(X, y)\n",
        "    t_eval_nystrom = time.time() - t0\n",
        "\n",
        "    pred_mean, _ = nystrom.predict(X_test)\n",
        "    rmse_nystrom = torch.sqrt(torch.mean((pred_mean - y_test)**2)).item()\n",
        "    outputs.append({\"method\" : \"Nystrom\", \"N\" : subset_size, \"Time\": t_eval_nystrom, \"RMSE\": rmse_nystrom})\n",
        "\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "RlDdD_eZcFC2"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}